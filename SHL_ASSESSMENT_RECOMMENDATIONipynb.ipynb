{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA73f9J8WY6f",
        "outputId": "14135c9d-b407-45e9-e3df-69b9e8aeaf60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            " * Running on https://865e-34-73-44-227.ngrok-free.app\n",
            " * If the URL gives 502, wait a minute and refresh\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 16:17:41] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 16:17:42] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 16:17:42] \"GET /evaluate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, render_template_string, request, jsonify\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# SHL API configuration (replace with actual SHL API details)\n",
        "SHL_API_URL = \"https://api.shl.com/assessments\"  # Example URL\n",
        "API_KEY = \"your_shl_api_key_here\"  # Should be stored securely in production\n",
        "\n",
        "# Mock data for demonstration (same as before)\n",
        "def get_shl_assessments():\n",
        "    \"\"\"Fetch SHL assessments from their API or use mock data\"\"\"\n",
        "    return [\n",
        "        {\n",
        "            \"id\": \"1\",\n",
        "            \"name\": \"Verify Interactive\",\n",
        "            \"description\": \"Cognitive ability test measuring verbal, numerical, and inductive reasoning\",\n",
        "            \"skills\": [\"cognitive\", \"verbal\", \"numerical\", \"reasoning\"],\n",
        "            \"job_level\": [\"entry\", \"mid\"],\n",
        "            \"duration\": 45,\n",
        "            \"popularity\": 8.5\n",
        "        },\n",
        "        {\n",
        "                \"id\": \"2\",\n",
        "                \"name\": \"OPQ32\",\n",
        "                \"description\": \"Personality questionnaire measuring behavioral preferences at work\",\n",
        "                \"skills\": [\"personality\", \"behavior\", \"work_preferences\"],\n",
        "                \"job_level\": [\"all\"],\n",
        "                \"duration\": 30,\n",
        "                \"popularity\": 9.0\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"3\",\n",
        "                \"name\": \"SJT Professional\",\n",
        "                \"description\": \"Situational Judgment Test for professional roles\",\n",
        "                \"skills\": [\"judgment\", \"decision_making\", \"professional_skills\"],\n",
        "                \"job_level\": [\"mid\", \"senior\"],\n",
        "                \"duration\": 25,\n",
        "                \"popularity\": 7.8\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"4\",\n",
        "                \"name\": \"Motivational Questionnaire\",\n",
        "                \"description\": \"Measures what drives and motivates individuals at work\",\n",
        "                \"skills\": [\"motivation\", \"drivers\", \"engagement\"],\n",
        "                \"job_level\": [\"all\"],\n",
        "                \"duration\": 20,\n",
        "                \"popularity\": 7.2\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"5\",\n",
        "                \"name\": \"Deductive Reasoning\",\n",
        "                \"description\": \"Measures logical reasoning and problem-solving skills\",\n",
        "                \"skills\": [\"cognitive\", \"logical_reasoning\", \"problem_solving\"],\n",
        "                \"job_level\": [\"entry\", \"mid\", \"senior\"],\n",
        "                \"duration\": 35,\n",
        "                \"popularity\": 8.1\n",
        "            }\n",
        "        ]\n",
        "\n",
        "def preprocess_data(assessments):\n",
        "    \"\"\"Convert assessments data to a format suitable for recommendation\"\"\"\n",
        "    processed = []\n",
        "    for assessment in assessments:\n",
        "        processed.append({\n",
        "            \"id\": assessment[\"id\"],\n",
        "            \"content\": f\"{assessment['name']} {assessment['description']} {' '.join(assessment['skills'])} {' '.join(assessment['job_level'])}\"\n",
        "        })\n",
        "    return processed\n",
        "\n",
        "# Recommendation engine\n",
        "class SHLRecommender:\n",
        "    def __init__(self):\n",
        "        self.assessments = get_shl_assessments()\n",
        "        self.processed_data = preprocess_data(self.assessments)\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform([item['content'] for item in self.processed_data])\n",
        "\n",
        "    def recommend_by_query(self, query, top_n=3):\n",
        "        \"\"\"Recommend assessments based on text query\"\"\"\n",
        "        query_vec = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
        "        related_indices = similarities.argsort()[::-1][:top_n]\n",
        "\n",
        "        recommendations = []\n",
        "        for idx in related_indices:\n",
        "          recommendations.append({\n",
        "    \"assessment\": self.assessments[idx],  # Corrected to match variable name\n",
        "    \"similarity_score\": float(similarities[idx])\n",
        "          })\n",
        "\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def recommend_by_job_level(self, job_level, top_n=3):\n",
        "        \"\"\"Recommend assessments based on job level\"\"\"\n",
        "        matched = [a for a in self.assessments if job_level in a['job_level']]\n",
        "        matched_sorted = sorted(matched, key=lambda x: x['popularity'], reverse=True)\n",
        "        return matched_sorted[:top_n]\n",
        "\n",
        "    def hybrid_recommendation(self, query=None, job_level=None, top_n=3):\n",
        "        \"\"\"Combine content-based and popularity-based recommendations\"\"\"\n",
        "        if query and job_level:\n",
        "            content_recs = self.recommend_by_query(query, top_n*2)\n",
        "            level_recs = self.recommend_by_job_level(job_level, top_n*2)\n",
        "\n",
        "            # Combine and deduplicate\n",
        "            combined = content_recs + [{\"assessment\": a, \"similarity_score\": 0} for a in level_recs]\n",
        "            unique_recs = {}\n",
        "            for rec in combined:\n",
        "                aid = rec['assessment']['id']\n",
        "                if aid not in unique_recs:\n",
        "                    unique_recs[aid] = rec\n",
        "                else:\n",
        "                    # Boost score if present in both\n",
        "                    unique_recs[aid]['similarity_score'] += 0.5\n",
        "\n",
        "            # Sort by combined score\n",
        "            sorted_recs = sorted(unique_recs.values(),\n",
        "                               key=lambda x: (x['similarity_score'], x['assessment']['popularity']),\n",
        "                               reverse=True)\n",
        "            return [r['assessment'] for r in sorted_recs[:top_n]]\n",
        "\n",
        "        elif query:\n",
        "            return [r['assessment'] for r in self.recommend_by_query(query, top_n)]\n",
        "        elif job_level:\n",
        "            return self.recommend_by_job_level(job_level, top_n)\n",
        "        else:\n",
        "            return sorted(self.assessments, key=lambda x: x['popularity'], reverse=True)[:top_n]\n",
        "\n",
        "# Evaluation metrics\n",
        "class EvaluationMetrics:\n",
        "    @staticmethod\n",
        "    def precision_at_k(recommendations, relevant_items, k):\n",
        "        \"\"\"Calculate precision at K\"\"\"\n",
        "        top_k = recommendations[:k]\n",
        "        relevant_set = set(relevant_items)\n",
        "        relevant_and_retrieved = [item for item in top_k if item['id'] in relevant_set]\n",
        "        return len(relevant_and_retrieved) / k\n",
        "\n",
        "    @staticmethod\n",
        "    def recall_at_k(recommendations, relevant_items, k):\n",
        "        \"\"\"Calculate recall at K\"\"\"\n",
        "        top_k = recommendations[:k]\n",
        "        relevant_set = set(relevant_items)\n",
        "        relevant_and_retrieved = [item for item in top_k if item['id'] in relevant_set]\n",
        "        return len(relevant_and_retrieved) / len(relevant_items) if relevant_items else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_average_precision(recommendations, relevant_items):\n",
        "        \"\"\"Calculate mean average precision\"\"\"\n",
        "        relevant_set = set(relevant_items)\n",
        "        average_precision = 0.0\n",
        "        num_relevant = 0\n",
        "\n",
        "        for k in range(1, len(recommendations)+1):\n",
        "            if recommendations[k-1]['id'] in relevant_set:\n",
        "                num_relevant += 1\n",
        "                precision_at_k = num_relevant / k\n",
        "                average_precision += precision_at_k\n",
        "\n",
        "        return average_precision / len(relevant_items) if relevant_items else 0\n",
        "\n",
        "    @staticmethod\n",
        "\n",
        "    def evaluate(recommender, test_cases):\n",
        "        \"\"\"Evaluate the recommender system with multiple test cases\"\"\"\n",
        "        results = {\n",
        "            'precision@3': [],\n",
        "            'recall@3': [],\n",
        "            'map': []\n",
        "        }\n",
        "\n",
        "        for case in test_cases:\n",
        "            query = case.get('query')\n",
        "            job_level = case.get('job_level')\n",
        "            relevant_ids = case['relevant_ids']\n",
        "\n",
        "            recommendations = recommender.hybrid_recommendation(query, job_level, top_n=5)\n",
        "            count = 0\n",
        "            recall_score = count/len(relevant_ids)\n",
        "\n",
        "            results['precision@3'].append(\n",
        "                EvaluationMetrics.precision_at_k(recommendations, relevant_ids, 3)\n",
        "            )\n",
        "            results['recall@3'].append(\n",
        "                EvaluationMetrics.recall_at_k(recommendations, relevant_ids, 3)\n",
        "            )\n",
        "            results['map'].append(\n",
        "                EvaluationMetrics.mean_average_precision(recommendations, relevant_ids)\n",
        "            )\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_metrics = {\n",
        "            'avg_precision@3': sum(results['precision@3']) / len(results['precision@3']),\n",
        "            'avg_recall@3': sum(results['recall@3']) / len(results['recall@3']),\n",
        "            'avg_map': sum(results['map']) / len(results['map'])\n",
        "        }\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "# Create test cases for evaluation\n",
        "def create_test_cases(assessments):\n",
        "    \"\"\"Create test cases for evaluation based on assessment data\"\"\"\n",
        "    # In a real scenario, these would come from user testing or historical data\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"query\": \"cognitive reasoning test\",\n",
        "            \"job_level\": \"entry\",\n",
        "            \"relevant_ids\": [\"1\", \"5\"]  # Verify Interactive and Deductive Reasoning\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"personality questionnaire\",\n",
        "            \"job_level\": \"all\",\n",
        "            \"relevant_ids\": [\"2\", \"4\"]  # OPQ32 and Motivational Questionnaire\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"professional skills assessment\",\n",
        "            \"job_level\": \"senior\",\n",
        "            \"relevant_ids\": [\"3\"]  # SJT Professional\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"logical reasoning\",\n",
        "            \"job_level\": \"mid\",\n",
        "            \"relevant_ids\": [\"1\", \"5\"],  # Verify Interactive and Deductive Reasoning\n",
        "            \"description\": \"Test for logical reasoning assessments\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"work motivation\",\n",
        "            \"job_level\": \"all\",\n",
        "            \"relevant_ids\": [\"4\"],  # Motivational Questionnaire\n",
        "            \"description\": \"Test for motivation assessments\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    return test_cases\n",
        "\n",
        "# Modified HTML template as a string (since we can't use templates folder in Colab)\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>SHL Assessment Recommender</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }\n",
        "        .container { display: flex; flex-direction: column; gap: 20px; }\n",
        "        .input-group { display: flex; flex-direction: column; gap: 5px; }\n",
        "        input, select, button { padding: 8px; font-size: 16px; }\n",
        "        button { background-color: #007bff; color: white; border: none; cursor: pointer; padding: 10px; }\n",
        "        button:hover { background-color: #0056b3; }\n",
        "        .results { margin-top: 20px; border-top: 1px solid #ddd; padding-top: 20px; }\n",
        "        .assessment-card { border: 1px solid #ddd; padding: 15px; margin-bottom: 10px; border-radius: 5px; }\n",
        "        .metrics { background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 30px; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>SHL Assessment Recommender</h1>\n",
        "\n",
        "        <div class=\"input-group\">\n",
        "            <label for=\"query\">What skills or attributes are you looking to assess?</label>\n",
        "            <input type=\"text\" id=\"query\" placeholder=\"e.g. cognitive skills, personality, etc.\">\n",
        "        </div>\n",
        "\n",
        "        <div class=\"input-group\">\n",
        "            <label for=\"job_level\">Job Level:</label>\n",
        "            <select id=\"job_level\">\n",
        "                <option value=\"\">Any level</option>\n",
        "                <option value=\"entry\">Entry Level</option>\n",
        "                <option value=\"mid\">Mid Level</option>\n",
        "                <option value=\"senior\">Senior Level</option>\n",
        "            </select>\n",
        "        </div>\n",
        "\n",
        "        <button onclick=\"getRecommendations()\">Get Recommendations</button>\n",
        "\n",
        "        <div class=\"results\" id=\"results\">\n",
        "            <!-- Recommendations will appear here -->\n",
        "        </div>\n",
        "\n",
        "        <div class=\"metrics\">\n",
        "            <h2>System Performance</h2>\n",
        "            <p>Our recommendation engine achieves the following evaluation metrics:</p>\n",
        "            <ul id=\"metrics-list\">\n",
        "                <li>Loading metrics...</li>\n",
        "            </ul>\n",
        "            <button onclick=\"refreshMetrics()\">Refresh Metrics</button>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        async function getRecommendations() {\n",
        "            const query = document.getElementById('query').value;\n",
        "            const job_level = document.getElementById('job_level').value;\n",
        "\n",
        "            const response = await fetch('/recommend', {\n",
        "                method: 'POST',\n",
        "                headers: {\n",
        "                    'Content-Type': 'application/json',\n",
        "                },\n",
        "                body: JSON.stringify({\n",
        "                    query: query,\n",
        "                    job_level: job_level\n",
        "                })\n",
        "            });\n",
        "\n",
        "            const data = await response.json();\n",
        "            displayResults(data.recommendations);\n",
        "        }\n",
        "\n",
        "        function displayResults(recommendations) {\n",
        "            const resultsDiv = document.getElementById('results');\n",
        "\n",
        "            if (!recommendations || recommendations.length === 0) {\n",
        "                resultsDiv.innerHTML = '<p>No recommendations found. Try a different query.</p>';\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            let html = '<h2>Recommended Assessments</h2>';\n",
        "\n",
        "            recommendations.forEach(assessment => {\n",
        "                html += `\n",
        "                <div class=\"assessment-card\">\n",
        "                    <h3>${assessment.name}</h3>\n",
        "                    <p>${assessment.description}</p>\n",
        "                    <p><strong>Skills assessed:</strong> ${assessment.skills.join(', ')}</p>\n",
        "                    <p><strong>Job levels:</strong> ${assessment.job_level.join(', ')}</p>\n",
        "                    <p><strong>Duration:</strong> ${assessment.duration} minutes</p>\n",
        "                </div>\n",
        "                `;\n",
        "            });\n",
        "\n",
        "            resultsDiv.innerHTML = html;\n",
        "        }\n",
        "\n",
        "        async function loadMetrics() {\n",
        "            const response = await fetch('/evaluate');\n",
        "            const data = await response.json();\n",
        "            const metrics = data.evaluation_metrics;\n",
        "\n",
        "            const metricsList = document.getElementById('metrics-list');\n",
        "            metricsList.innerHTML = `\n",
        "                <li><strong>Precision@3:</strong> ${(metrics.avg_precision_at_3 * 100).toFixed(1)}%</li>\n",
        "                <li><strong>Recall@3:</strong> ${(metrics.avg_recall_at_3 * 100).toFixed(1)}%</li>\n",
        "                <li><strong>Mean Average Precision:</strong> ${(metrics.avg_map * 100).toFixed(1)}%</li>\n",
        "            `;\n",
        "        }\n",
        "\n",
        "        function refreshMetrics() {\n",
        "            loadMetrics();\n",
        "        }\n",
        "\n",
        "        // Load metrics when page loads\n",
        "        window.onload = loadMetrics;\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "# ... [Keep all the other routes from the original code: /recommend, /evaluate]\n",
        "\n",
        "@app.route('/recommend', methods=['POST'])\n",
        "def recommend():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get('query')\n",
        "        job_level = data.get('job_level')\n",
        "\n",
        "        recommender = SHLRecommender()\n",
        "        recommendations = recommender.hybrid_recommendation(query, job_level)\n",
        "\n",
        "        return jsonify({\n",
        "            \"status\": \"success\",\n",
        "            \"recommendations\": recommendations\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"status\": \"error\",\n",
        "            \"message\": str(e)\n",
        "        }), 500\n",
        "@app.route('/evaluate')\n",
        "def evaluate():\n",
        "    recommender = SHLRecommender()\n",
        "    test_cases = create_test_cases(recommender.assessments)\n",
        "    metrics = EvaluationMetrics.evaluate(recommender, test_cases)\n",
        "\n",
        "    return jsonify({\n",
        "        \"status\": \"success\",\n",
        "        \"evaluation_metrics\": metrics\n",
        "    })\n",
        "\n",
        "# @app.route('/evaluate', methods=['GET'])\n",
        "# def evaluate():\n",
        "#     \"\"\"Endpoint to run evaluation and return metrics\"\"\"\n",
        "#     try:\n",
        "#         recommender = SHLRecommender()\n",
        "#         test_cases = create_test_cases(recommender.assessments)\n",
        "#         metrics = EvaluationMetrics.evaluate(recommender, test_cases)\n",
        "\n",
        "#         # Format results for better display\n",
        "#         formatted_metrics = {\n",
        "#             \"Precision@3\": f\"{metrics['avg_precision@3']*100:.1f}%\",\n",
        "#             \"Recall@3\": f\"{metrics['avg_recall@3']*100:.1f}%\",\n",
        "#             \"Precision@5\": f\"{metrics['avg_precision@5']*100:.1f}%\",\n",
        "#             \"Recall@5\": f\"{metrics['avg_recall@5']*100:.1f}%\",\n",
        "#             \"MAP\": f\"{metrics['mean_average_precision']*100:.1f}%\",\n",
        "#             \"MRR\": f\"{metrics['mean_reciprocal_rank']*100:.1f}%\",\n",
        "#             \"Test Cases\": len(test_cases)\n",
        "#         }\n",
        "\n",
        "#         return jsonify({\n",
        "#             \"status\": \"success\",\n",
        "#             \"metrics\": formatted_metrics,\n",
        "#             \"raw_metrics\": metrics  # For debugging\n",
        "#         })\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\n",
        "#             \"status\": \"error\",\n",
        "#             \"message\": str(e)\n",
        "#         }), 500\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # Install ngrok if not already installed\n",
        "#     !pip install pyngrok\n",
        "\n",
        "#     # Start ngrok tunnel\n",
        "\n",
        "\n",
        "#     from pyngrok import ngrok\n",
        "#     public_url = ngrok.connect(5000).public_url\n",
        "#     print(f\" * Running on {public_url}\")\n",
        "\n",
        "#     # Run the app\n",
        "#     app.run()\n",
        "import subprocess\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Install pyngrok\n",
        "    subprocess.run(['pip', 'install', 'pyngrok'], check=True)\n",
        "\n",
        "    # Set your ngrok authtoken (replace with your actual token)\n",
        "    NGROK_AUTH_TOKEN = \"2waVPVr8AuX0MeP60cxIWdiUas3_GFBUuUg3tFuJ65oPHgQu\"  # e.g., \"2AbC...xyz\"\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "    try:\n",
        "        # Start ngrok tunnel\n",
        "        public_url = ngrok.connect(5000).public_url\n",
        "        print(f\" * Running on {public_url}\")\n",
        "        print(\" * If the URL gives 502, wait a minute and refresh\")\n",
        "\n",
        "        # Run the app\n",
        "        app.run()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Running without ngrok (only accessible within Colab)\")\n",
        "        app.run()"
      ]
    }
  ]
}